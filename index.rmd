---
title: "Davinci"
runtime: shiny
output:
    html_document:
        highlight: espresso
---

<div align="center">
  <img src="https://upload.wikimedia.org/wikipedia/commons/2/2d/Vitruvian-Icon-Gray.png" style="height:120px;padding:10px"/>
  <img src="https://www.js-soft.com/assets/template_img/js-soft.png" style="height:90px;padding:10px"/>
</div>

<!-- Visualize the MNIST dataset using a shiny app. -->
```{r visualizing-mnist, echo=FALSE, message=FALSE, warning=FALSE}

# Setup data, variables and functions
library(knitr)
library(keras)
library(shiny)
library(tidyverse)
mnist <- dataset_mnist()
 
# Normalize and flatten 28x28 images using row-major layout (which is used by tensorflow)
x.train <- array_reshape(mnist$train$x, c(nrow(mnist$train$x), 784)) / 255
x.test <- array_reshape(mnist$test$x, c(nrow(mnist$test$x), 784)) / 255

# one-hot-encode labels
y.train <- to_categorical(mnist$train$y, 10)
y.test <- to_categorical(mnist$test$y, 10)

# Define method to draw mnist images.
draw.pixmap <- function(pixels) {
                y_ <- c()
                for (ii in 28:1) { y_ <- c(y_, rep(ii, 28)) }
                ggplot() +
                  geom_raster(aes(x = rep(1:28, 28), y = y_, fill = pixels)) +
                  coord_cartesian(xlim=c(1,28), ylim=c(1,28)) +
                  theme_void() +
                  theme(legend.position="none")
                }

# Define user interface
ui <- fluidPage(

  fluidRow(
    column(4, align="center",
      splitLayout(cellWidths = c("85%", "15%"),
        sliderInput('test.index', label='Test sample', min=1, max=10000, value=1, width="90%"),
        code(textOutput(outputId = 'label.test'))
      ),
      plotOutput(outputId = 'image.test')
    ),
    column(4, align="center",
      splitLayout(cellWidths = c("85%", "15%"),
        sliderInput('train.index', label='Training sample', min=1, max=60000, value=1, width="90%"),
        code(textOutput(outputId = 'label.train'))
      ),
      plotOutput(outputId = 'image.train')
    ),
    column(4, align="center",
      plotOutput(outputId = 'mnist.numbers.barchart')
    )
  )
)

# Define reactive outputs
server <- function(input, output) {

  # Pixmap of test image
  output$image.test <- renderPlot({

    img.out <- draw.pixmap(x.test[input$test.index,])

    # information can be passed explicitly via a list
    list(src = img.out)
  }, width = 140, height = 140 )
  
  # Pixmap of training image
  output$image.train <- renderPlot({

    draw.pixmap(x.train[input$train.index,])
  }, width = 140, height = 140)

  # Label of test image
  output$label.test <- renderText({

    mnist$test$y[[input$test.index]]
  })

  # Label of training image
  output$label.train <- renderText({

    mnist$train$y[[input$train.index]]
  })

  # Barchart of the number's distributions in the MNIST dataset
  output$mnist.numbers.barchart <- renderPlot({

    my.df <- bind_rows(tibble(labels=mnist$test$y), 
                       tibble(labels=mnist$train$y),
                       .id="Group") %>%
             mutate(Group=case_when(Group=="1" ~ "test", TRUE ~ "training")) %>%
             mutate(Group=factor(Group), labels=as.integer(labels))

    ggplot(my.df) +
      geom_bar(aes(x=labels, y=..prop.., fill=Group, color=Group), position="dodge") +
      # must create xlabels manually as I cannot declare labels as factors (bugs). See
      # https://stackoverflow.com/questions/56360383/geom-bar-changes-behavior-when-variable-is-factor
      scale_x_continuous(breaks=0:9, labels=as.character(0:9)) +
      scale_y_continuous(breaks=c()) +
      labs(x = "", y = "Proportion (rel.)", title = "Distribution of numbers in MNIST") +
      theme(legend.position=c(0.95, 0.05),
            legend.justification=c("right","bottom"),
            legend.background=element_rect(color=1),
            plot.title=element_text(hjust = 0.5)
      )
  })
}

# Run the app
shinyApp(ui = ui, server = server)

```

<!-- visualize training progress using a shiny app. -->
```{r keras-test-1, echo=FALSE}

# Debugging log wrapper
debug.log <- function(...) {

  # Green & bold
  cat("\033[38;2;0;255;0m", ..., "\033[39;49m")
}

# Define user interface
ui.training <-
  fluidPage(
    fluidRow(
      column(4, align="center",
        sliderInput("epoch.increment", label="Numer of Epochs to Train", value=4, min=1, max=32, step=1),
        splitLayout(cellWidths=c("65%", "35%"),
          actionButton("run.button", label="Run Training", width="100%"),
          actionButton("reset.button", label="Reset", width="100%")
        ),
        # TODO: Properly concatenate the static prefix and the dynamic counter value
        splitLayout(cellWidths=c("75%", "25%"),
          h3("Total epochs: "),
          h3(textOutput("epoch.counter"))
        ),
        br(), br(),
        checkboxInput("loss.option", label="Display Loss", value=TRUE)
      ),
      column(6, align="center", plotOutput("plot.history")
      )
    )
  )

# define reactive outputs
server.training <- function(input, output){

  # define function to (re-)initialize model
  initialize_model <- function() {

    # set up model layers
    model <- keras_model_sequential(name="MNIST Classifier") %>%
              layer_dense(units=2, activation='relu', input_shape=c(784), name="Input-Layer") %>%
              # layer_dropout(rate=0.4, name="H1") %>%
              # layer_dense(units=128, activation='relu', name="H2") %>%
              # layer_dropout(rate=0.3, name="H3") %>%
              layer_dense(units=10, activation='softmax', name="Output-Layer")
    # define loss function and optimizer ('compile' the model)
    model %>% compile(loss = 'categorical_crossentropy',
                      metrics = c('accuracy'),
                      optimizer = optimizer_rmsprop())

    return(model)
  }

  # define reactive parameters
  params = reactiveValues(epoch.counter = 0)

  # define reactive function performing training and returning training history as wide-format tibble.
  training_history <- eventReactive(params$epoch.counter, {
    debug.log("training_history() called\n", 
              "\tepoch.counter = ", params$epoch.counter, "\n",
              "\tepoch.counter.prev. = ", get0("epoch.counter.prev.", ifnotfound = "(not defined)"), "\n")
    
    if(params$epoch.counter == 0){
      debug.log("\tinit pending ..\n")

      # we use static variables which are incrementally changed with each function call. Unfortunately R does not have
      # function-scope static variables hence we're forced to use globals.
      m. <<- initialize_model() # our model
      h. <<- tibble()           # training history
      epoch.counter.prev. <<- 0 # prev value of the epoch.counter

      return(h.)
    }

    # perform trainig. Note that the model is mutated by 'fit()'
    fit.result <- fit(m., x.train, y.train,
                      initial_epoch = epoch.counter.prev.,
                      epochs = params$epoch.counter,
                      validation_split = 0.2,
                      batch_size = 12000)


    # append newly gathered values. The column names are deducted automatically
    dh <- fit.result$metrics %>% as_tibble
    h. <<- bind_rows(h., dh)

    epoch.counter.prev. <<- params$epoch.counter

    return(h.)
    # run the actual training within a progress environmente
    # withProgress(message = 'Training ...', value = 0, {


    #   # perform training for the specified number of epochs. We use a loop to obtain explicit control over the
    #   # progress of the training.
    #   for (ii in 1:epoch.increment){

    #     # run an epoch of training
    #     training.history <- fit(params$model, x.train, y.train,
    #                             initial_epoch = current.epoch,
    #                             epochs = current.epoch + 1,
    #                             batch_size = 10000)


    #     # display progress bar
    #     incProgress(1/epoch.increment, detail = paste0("Training epoch ", ii, "/", epoch.increment))
    #   }
    # })
  })

  # render training history plot
  output$plot.history <- renderPlot({
    debug.log("output$plot.history called\n")

    # Create explicit dependency to the function performing the model's training. The input format is expected to be
    # a wide-format tibble consisting of all the metrics to be displayed.
    training.history <- training_history()

    if(nrow(training.history) == 0) {
      return(ggplot())
    }

    # Append an index column to training history and transform into long-format
    plot.data <- training.history %>%
      # Rename metrics for proper plot labels
      rename_all(function(name) { 
        case_when(name == "accuracy" ~ "Accuracy [Test Set]",
                  name == "loss" ~ "Loss [Test Set]",
                  name == "val_accuracy" ~ "Accuracy [Valid. Set]",
                  name == "val_loss" ~ "Loss [Valid. Set]") }) %>%
      mutate(Epoch = 1:nrow(training.history)) %>% 
      gather(key = "Metric", value = "Value", -Epoch)

    # Optionally filter out losses
    if(input$loss.option == FALSE ) {
      plot.data <- plot.data %>%
        filter(Metric != "Loss [Test Set]") %>%
        filter(Metric != "Loss [Valid. Set]")
    }

    plot.graph <- ggplot(plot.data) +
      geom_point(aes(x = Epoch, y = Value, color = Metric)) +
      scale_x_continuous(minor_breaks=NULL)

    return(plot.graph)
  })

  # render epoch counter
  output$epoch.counter <- renderText({

    # Print the current epoch. The value is retrieved from a reactive variable and thus updates by itself. No
    # explicit dependency on the button is required.
    return(params$epoch.counter)
  })

  # 'reset.button' callback: Reset epoch counter
  observeEvent(input$reset.button, {
    debug.log("reset.button pressed\n")

    params$epoch.counter = 0
  })

  # 'run.button' callback: Increment epoch counter
  observeEvent(input$run.button, {
    debug.log("run.button pressed\n")

    params$epoch.counter <- params$epoch.counter + input$epoch.increment
  })
}

# run the app
shinyApp(ui = ui.training, server = server.training)

# TODO: Display false positives
#model %>% evaluate(x.test, y.test)

# Check out false positives
#predictions <- model %>% predict_classes(x.test)
```

# Tensorflow Übersicht 

* Generische Bibliothek für *datenstromorientierte Numerik*
  + kann für ML & NN verwendet werden
* Hardware-agnostisch (CPU/GPU/ASIC)
* Viele Schnittstellen zu C++-Runtime (Python et al.)
* Out-of-Core fähig (!)
* Open-Source (!)
  + *'Macht-was-ihr-wollt'* Lizens (Apache)
* Sehr umfangreich u. flexibel


## TensorFlow Interfaces

* TF Core API: Low-Level TF Interface ('Bare Metal')
  + Direkte Manipulation des TF-Graphen
  + manuelle Entwicklung v. Frameworks/Tools
* Keras API:  High-Level TF API für NN
  + Zusammensetzen vordefinierter Layer-Typen
* Estimator API: High-Level TF API für klassisches ML

## Material

[Machine Learning with R and Tensorflow](https://www.youtube.com/watch?v=atiYXm7JZv0)
[keras - Deep Learning in R](https://www.datacamp.com/community/tutorials/keras-r-deep-learning)

## Quarantäne

* Gute Entwicklungsumgebung ist dadurch charakterisiert, dass einfache Absichten mit gleichermaßen einfachem Aufwand realisiert werden können. Falls Feature nicht mittels dezidiertem Parameter konfiguriert werden kann, wie groß ist DIY-Aufwand?
  + Negativbeispiel Windows 
    * VDesktop Wechsel braucht 100 Zeilen AHK Code.
    * Keine Automatische Paketverwaltung
    * *Downloader*-Programme
