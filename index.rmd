---
title: "Davinci"
runtime: shiny
output:
    html_document:
        highlight: espresso
---

<div align="center">
  <img src="https://upload.wikimedia.org/wikipedia/commons/2/2d/Vitruvian-Icon-Gray.png" style="height:120px;padding:10px"/>
  <img src="https://www.js-soft.com/assets/template_img/js-soft.png" style="height:90px;padding:10px"/>
</div>

<!-- Visualize the MNIST dataset using a shiny app. -->
```{r visualizing-mnist, echo=FALSE, message=FALSE, warning=FALSE}

# Setup data, variables and functions
library(knitr)
library(keras)
library(shiny)
library(tidyverse)
mnist <- dataset_mnist()
 
# Normalize and flatten 28x28 images using row-major layout (which is used by tensorflow)
x.train <- array_reshape(mnist$train$x, c(nrow(mnist$train$x), 784)) / 255
x.test <- array_reshape(mnist$test$x, c(nrow(mnist$test$x), 784)) / 255

# one-hot-encode labels
y.train <- to_categorical(mnist$train$y, 10)
y.test <- to_categorical(mnist$test$y, 10)

# Define method to draw mnist images.
draw.pixmap <- function(pixels) {
                y_ <- c()
                for (ii in 28:1) { y_ <- c(y_, rep(ii, 28)) }
                ggplot() +
                  geom_raster(aes(x = rep(1:28, 28), y = y_, fill = pixels)) +
                  coord_cartesian(xlim=c(1,28), ylim=c(1,28)) +
                  theme_void() +
                  theme(legend.position="none")
                }

# Define user interface
ui <- fluidPage(

  fluidRow(
    column(4, align="center",
      splitLayout(cellWidths = c("85%", "15%"),
        sliderInput('test.index', label='Test sample', min=1, max=10000, value=1, width="90%"),
        code(textOutput(outputId = 'label.test'))
      ),
      plotOutput(outputId = 'image.test')
    ),
    column(4, align="center",
      splitLayout(cellWidths = c("85%", "15%"),
        sliderInput('train.index', label='Training sample', min=1, max=60000, value=1, width="90%"),
        code(textOutput(outputId = 'label.train'))
      ),
      plotOutput(outputId = 'image.train')
    ),
    column(4, align="center",
      plotOutput(outputId = 'mnist.numbers.barchart')
    )
  )
)

# Define reactive outputs
server <- function(input, output) {

  # Pixmap of test image
  output$image.test <- renderPlot({

    img.out <- draw.pixmap(x.test[input$test.index,])

    # information can be passed explicitly via a list
    list(src = img.out)
  }, width = 140, height = 140 )
  
  # Pixmap of training image
  output$image.train <- renderPlot({

    draw.pixmap(x.train[input$train.index,])
  }, width = 140, height = 140)

  # Label of test image
  output$label.test <- renderText({

    mnist$test$y[[input$test.index]]
  })

  # Label of training image
  output$label.train <- renderText({

    mnist$train$y[[input$train.index]]
  })

  # Barchart of the number's distributions in the MNIST dataset
  output$mnist.numbers.barchart <- renderPlot({

    my.df <- bind_rows(tibble(labels=mnist$test$y), 
                       tibble(labels=mnist$train$y),
                       .id="Group") %>%
             mutate(Group=case_when(Group=="1" ~ "test", TRUE ~ "training")) %>%
             mutate(Group=factor(Group), labels=as.integer(labels))

    ggplot(my.df) +
      geom_bar(aes(x=labels, y=..prop.., fill=Group, color=Group), position="dodge") +
      # must create xlabels manually as I cannot declare labels as factors (bugs). See
      # https://stackoverflow.com/questions/56360383/geom-bar-changes-behavior-when-variable-is-factor
      scale_x_continuous(breaks=0:9, labels=as.character(0:9)) +
      scale_y_continuous(breaks=c()) +
      labs(x = "", y = "Proportion (rel.)", title = "Distribution of numbers in MNIST") +
      theme(legend.position=c(0.95, 0.05),
            legend.justification=c("right","bottom"),
            legend.background=element_rect(color=1),
            plot.title=element_text(hjust = 0.5)
      )
  })
}

# Run the app
shinyApp(ui = ui, server = server)

```

<!-- visualize training progress using a shiny app. -->
```{r keras-test-1, echo=FALSE}

# Debugging log wrapper
debug.log <- function(...) {

  # Red & bold
  cat("\033[1;31m", ..., "\033[0m")
}

# Define user interface
ui.training <-
  fluidPage(
    fluidRow(
      column(4, align="center",
        sliderInput("increment", label="Numer of Epochs to Train", value=1, min=1, max=32, step=1),
        actionButton("run.button", label="Run Training", width="100%"),
        br(), br(),
        actionButton("reset.button", label="Reset Model", width="100%"),
        h1(textOutput("epoch.counter"))
      ),
      column(6, align="center", plotOutput("plot.history")
      )
    )
  )

# define reactive outputs
server.training <- function(input, output){

  initialize_model <- function() {

    # set up model layers
    model <- keras_model_sequential(name="MNIST Classifier") %>%
              layer_dense(units=256, activation='relu', input_shape=c(784), name="Input-Layer") %>%
              layer_dropout(rate=0.4, name="H1") %>%
              layer_dense(units=128, activation='relu', name="H2") %>%
              layer_dropout(rate=0.3, name="H3") %>%
              layer_dense(units=10, activation='softmax', name="Output-Layer")
    # define loss function and optimizer ('compile' the model)
    model %>% compile(loss = 'categorical_crossentropy',
                      metrics = 'accuracy',
                      optimizer = optimizer_rmsprop())

    return(model)
  }

  accuracy <- numeric() # vector of accuracies obtained after each epoch

  # create container for reactive objects. Values will be intialized on-demand
  params <- reactiveValues(current.epoch = NA,
                           model = initialize_model())

  # epoch counter
  output$epoch.counter <- renderText({

    # Print the current epoch. The value is retrieved from a reactive variable and thus updates by itself. No
    # explicit dependency on the button is required.
    return(params$current.epoch)
  })

  # 'reset.button' callback: Reset model and associated variables
  observeEvent(input$reset.button, {

    debug.log("Reset button pressed\n")
    params$model <- initialize_model()
    params$current.epoch <- NA
    accuracy <<- numeric()
  })

  # training history plot
  output$plot.history <- renderPlot({

    # create an explicit dependecy on the button. This will cause this output to rendered whenver the button is
    # pressed.
    input$run.button

    # set up plot skeleton and initialize values if this is the initial run.
    plt <- ggplot()                             +
             scale_y_log10(limits=c(0.10,1))    +
             scale_x_continuous(limits=c(0,32)) +
             labs(x = "Epoch", y = "Accuracy")
    if(is.na(isolate(params$current.epoch))){
      params$current.epoch <- 0
      return(plt)
    }
  
    # run the actual training within a progress environment
    withProgress(message = 'Training ...', value = 0, {

      # 'isolate' causes no dependency to be created between the input and its consuming output. Otherwise,
      # every change to the input field would cause a re-rendering of the output. We just want to render
      # the plot when the button is pressed.
      increment     <- isolate(input$increment)
      current.epoch <- isolate(params$current.epoch)

      # perform training for the specified number of epochs. We use a loop to obtain explicit control over the
      # progress of the training.
      for (ii in 1:increment){

        # run an epoch of training
        training.history <- fit(params$model, x.train, y.train,
                                initial_epoch = current.epoch,
                                epochs = current.epoch + 1,
                                batch_size = 10000)

        # Increment epoch
        current.epoch = current.epoch + 1

        # append resulting accuracy
        accuracy <<- c(accuracy, training.history$metrics$accuracy)

        # display progress bar
        incProgress(1/increment, detail = paste0("Training epoch ", ii, "/", increment))
      }
    })

    # draw the data onto the frame created above
    plt <- plt + geom_point(aes(x = 1:current.epoch, y = accuracy))

    # increment the epoch counter. We avoid a dependency on the parameter by wrapping the read access in the
    # 'isolate' environment (omitting this would cause infinite recursion). Write accesses don't create dependencies,
    # hence the LHS of the assignment needs no 'isolate'.
    params$current.epoch <- isolate(params$current.epoch) + increment

    return(plt)
 })
}

# run the app
shinyApp(ui = ui.training, server = server.training)

# TODO: Display false positives
#model %>% evaluate(x.test, y.test)

# Check out false positives
#predictions <- model %>% predict_classes(x.test)
```

# Tensorflow Übersicht 

* Generische Bibliothek für *datenstromorientierte Numerik*
  + kann für ML & NN verwendet werden
* Hardware-agnostisch (CPU/GPU/ASIC)
* Viele Schnittstellen zu C++-Runtime (Python et al.)
* Out-of-Core fähig (!)
* Open-Source (!)
  + *'Macht-was-ihr-wollt'* Lizens (Apache)
* Sehr umfangreich u. flexibel


## TensorFlow Interfaces

* TF Core API: Low-Level TF Interface ('Bare Metal')
  + Direkte Manipulation des TF-Graphen
  + manuelle Entwicklung v. Frameworks/Tools
* Keras API:  High-Level TF API für NN
  + Zusammensetzen vordefinierter Layer-Typen
* Estimator API: High-Level TF API für klassisches ML

## Material

[Machine Learning with R and Tensorflow](https://www.youtube.com/watch?v=atiYXm7JZv0)
[keras - Deep Learning in R](https://www.datacamp.com/community/tutorials/keras-r-deep-learning)

## Quarantäne

* Gute Entwicklungsumgebung ist dadurch charakterisiert, dass einfache Absichten mit gleichermaßen einfachem Aufwand realisiert werden können. Falls Feature nicht mittels dezidiertem Parameter konfiguriert werden kann, wie groß ist DIY-Aufwand?
  + Negativbeispiel Windows 
    * VDesktop Wechsel braucht 100 Zeilen AHK Code.
    * Keine Automatische Paketverwaltung
    * *Downloader*-Programme
